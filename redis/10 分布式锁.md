## 分布式锁

## 分布式锁的要求

对于一个分布式锁有以下几个基本要求：

1. 独占性：不管有多少个节点进行抢锁，一次只允许一个线程持有锁
2. 高可用：不能因为一些问题导致锁功能不可用了
3. 防死锁：分布式情况下，如果持有锁的服务挂了，锁需要能够自动释放
4. 不乱抢：锁的释放只能由持有锁的线程进行，其他线程不能进行释放操作
5. 可重入：如果锁不可重入，一个线程去使用都有可能出现死锁

## 分布式锁的分类

分布式锁的分类由其实现方式可分CP锁、AP锁（CAP理论）。

使用数据库也可以实现分布式锁（但是一般没人用，性能太低，高并发直接打死）

使用redis实现，由于redis的高可用架构是AP的，所以用redis实现的分布式锁是AP锁

使用zookeeper实现，由于zookeeper的高可用架构是CP的，所以用zookeeper实现的锁是CP锁

AP锁的效率比CP锁高，CP锁的安全性比AP高。

在一般的互联网企业使用AP锁的多，金融行业使用CP锁多

## redisson锁实现

redisson锁实现：

1. 基于redis的lua脚本互斥实现加锁逻辑
2. 通过hincrby自增实现锁的可重入
3. 通过为key设置过期时间防止死锁
4. 通过后台看门狗线程实现过期时间的延长实现锁的续命，防止业务未执行完成超时导致锁释放
5. 未抢到锁的线程监听并阻塞在该锁的发布订阅上，等待锁的释放一旦释放则再次进行抢锁

## redisson关键源码

### lua加锁逻辑

可以看到redisson的锁实现是基于hash的，跟我们平时用的setnx原子的设置单个key来实现分布式锁有些不同。lua脚本保证了这些操作是原子的。

```tex
## lua脚本
## 一进来判断是否存在这个锁，如果不存在则直接设置锁
## 同时制定持有线程为当前线程，状态为1
## 设置过期时间
## 由于redis与内存交互是单线程的，此处即使不是setnx也不会有安全性问题
## 下一个脚本被处理时，锁已经被拿走了，不进入这个分支
"if (redis.call('exists', KEYS[1]) == 0) then " +
    "redis.call('hset', KEYS[1], ARGV[2], 1); " +
    "redis.call('pexpire', KEYS[1], ARGV[1]); " +
    "return nil; " +
    "end; " +
## 判断是不是自己持有锁，如果是，则对状态进行加一操作（可重入）
## 刷新过期时间
"if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
    "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
    "redis.call('pexpire', KEYS[1], ARGV[1]); " +
    "return nil; " +
    "end; " +
## 前面两个if都不走，说明锁被其他线程持有，无法获取锁
## 返回锁的持续时间
"return redis.call('pttl', KEYS[1]);"
```

### 看门狗的设置

```java
private <T> RFuture<Long> tryAcquireAsync(long leaseTime, TimeUnit unit, long threadId) {
  // 只有使用默认的占用时间才会使用看门狗续期，如果指定了占用时间则只是通过脚本进行锁占用
  // 时间到则自动释放
        if (leaseTime != -1) {
            return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG);
        }
  // 通过脚本进行抢锁，返回一个future
        RFuture<Long> ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG);
  // 如果抢锁成功则执行onComplete中的逻辑
        ttlRemainingFuture.onComplete((ttlRemaining, e) -> {
            if (e != null) {
                return;
            }

            // lock acquired
            if (ttlRemaining == null) {
              // 缓存续命关键逻辑
                scheduleExpirationRenewal(threadId);
            }
        });
        return ttlRemainingFuture;
    }
```

```java
// 保存这个线程的信息到EXPIRATION_RENEWAL_MAP中
private void scheduleExpirationRenewal(long threadId) {
        ExpirationEntry entry = new ExpirationEntry();
        ExpirationEntry oldEntry = EXPIRATION_RENEWAL_MAP.putIfAbsent(getEntryName(), entry);
        if (oldEntry != null) {
            oldEntry.addThreadId(threadId);
        } else {
            entry.addThreadId(threadId);
          // 续期
            renewExpiration();
        }
    }

private void renewExpiration() {
        ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName());
        if (ee == null) {
            return;
        }
        
        Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() {
            @Override
            public void run(Timeout timeout) throws Exception {
                ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName());
                if (ent == null) {
                    return;
                }
                Long threadId = ent.getFirstThreadId();
                if (threadId == null) {
                    return;
                }
                // 调用lua脚本为锁的hash设置过期时间
              	// 这个过期时间又会被刷新为30s
              	// 默认情况下是30s，可以通过LockWatchdogTimeout修改
                RFuture<Boolean> future = renewExpirationAsync(threadId);
                future.onComplete((res, e) -> {
                    if (e != null) {
                        log.error("Can't update lock " + getName() + " expiration", e);
                        return;
                    }
                    // 再次调用
                    // reschedule itself
                    renewExpiration();
                });
            }
          // 注意，超过LockWatchdogTimeout的1/3则会进行一次续期，每次续期都把时间加满
          // 默认情况下每过10s，将锁占用时间刷新为30s
        }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS);
        
        ee.setTimeout(task);
    }
```

### 抢锁逻辑

1. 尝试抢锁，如果抢锁成功，则设置看门狗，并进入临界区
2. 如果抢锁失败则阻塞等待锁释放的release（如果订阅的Channel有释放锁的消息到来则release），阻塞时间为锁等待时间。
3. 如果在锁等待超时之前释放，则去再次去尝试获取锁，如果没有获取到锁则又进行阻塞等待release
4. 阻塞的时间为等待时间和锁释放时间中的较小一个，每次尝试获取锁或者着阻塞操作之后都要减去操作花费的时间
5. 在退出函数前会将订阅取消

```java
public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException {
  			// 锁的等待时间
        long time = unit.toMillis(waitTime);
        long current = System.currentTimeMillis();
        long threadId = Thread.currentThread().getId();
  			// 尝试通过lua脚本获取锁，如果获取锁成功，则设置看门狗
  			// 如果没有获取到锁则会返回上一个人的锁的过期时间，方便后面进行阻塞，避免不必要的资源浪费
        Long ttl = tryAcquire(leaseTime, unit, threadId);
        // lock acquired
        if (ttl == null) {
            return true;
        }
        
  			// 抢锁花去了一部分时间，看看超没超过锁的等待时间
  			// 如果超过了则不用等下去了
        time -= System.currentTimeMillis() - current;
        if (time <= 0) {
            acquireFailed(threadId);
            return false;
        }
        
        current = System.currentTimeMillis();
  			// 由于前面没抢到锁，订阅锁释放的Channel
  			// 其实就是在一个Semaphore上阻塞，等待release
  			// 如果锁释放则会release
        RFuture<RedissonLockEntry> subscribeFuture = subscribe(threadId);
  			// 阻塞在该Channel上，并设置等待时间，如果是阻塞时间到了还没有消息过来则不必要等了
  			// 取消这个订阅并返回
        if (!await(subscribeFuture, time, TimeUnit.MILLISECONDS)) {
            if (!subscribeFuture.cancel(false)) {
                subscribeFuture.onComplete((res, e) -> {
                    if (e == null) {
                        unsubscribe(subscribeFuture, threadId);
                    }
                });
            }
            acquireFailed(threadId);
            return false;
        }

 				// 到这里说明在锁等待时间到达前，上一任持有锁的人就释放了锁
        try {
          	// 减去前面等待花的时间，如果时间不够也在这里返回
          	// 可能有人认为此处如果正好是锁释放了，然后时间又不够了
          	// 在这里返回会导致订阅没法取消，其实不然，下面有个finally块
            time -= System.currentTimeMillis() - current;
            if (time <= 0) {
                acquireFailed(threadId);
                return false;
            }
        		// 进入循环抢锁逻辑
            while (true) {
                long currentTime = System.currentTimeMillis();
              	// 再次尝试获取锁
                ttl = tryAcquire(leaseTime, unit, threadId);
                // lock acquired
                if (ttl == null) {
                    return true;
                }
								// 减去花费时间
                time -= System.currentTimeMillis() - currentTime;
                if (time <= 0) {
                    acquireFailed(threadId);
                    return false;
                }

              	// 在Semaphore上等待锁释放的release
              	// 等待时间设置为过期时间和锁等待时间中较小的一个
                // waiting for message
                currentTime = System.currentTimeMillis();
                if (ttl >= 0 && ttl < time) {
                    getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS);
                } else {
                    getEntry(threadId).getLatch().tryAcquire(time, TimeUnit.MILLISECONDS);
                }
								// 被唤醒后再次减去阻塞花费的时间
                time -= System.currentTimeMillis() - currentTime;
                if (time <= 0) {
                    acquireFailed(threadId);
                    return false;
                }
            }
        } finally {
          	// 兜底解除订阅
            unsubscribe(subscribeFuture, threadId);
        }
    }
```

### lua解锁逻辑

```tex
## 如果这个锁不是自己的则不进行解锁返回nil
"if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then " +
"return nil;" +
"end; " +
## 锁计数器进行减一操作，因为锁是可重入的
"local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); " +
## 如果减一后还是大于0，表示锁还是被持有着，给锁续一下期
## 返回0
"if (counter > 0) then " +
	"redis.call('pexpire', KEYS[1], ARGV[2]); " +
  "return 0; " +
## 否则的话，说明真正需要进行解锁
## 删除对应的key，并发布锁释放的消息
## 返回1
"else " +
  "redis.call('del', KEYS[1]); " +
  "redis.call('publish', KEYS[2], ARGV[1]); " +
  "return 1; "+
  "end; " +
## 这步不会走过来
"return nil;"
```

### 解锁代码

```java
public RFuture<Void> unlockAsync(long threadId) {
        RPromise<Void> result = new RedissonPromise<Void>();
  			// 尝试使用lua脚本解锁
        RFuture<Boolean> future = unlockInnerAsync(threadId);
				// 脚本执行完成之后根据返回值来进行后续操作
        future.onComplete((opStatus, e) -> {
          	// 发生异常了，那么取消看门狗，返回失败
            if (e != null) {
                cancelExpirationRenewal(threadId);
                result.tryFailure(e);
                return;
            }
						// 如果返回的是null，代表脚本返回了nil
          	// 说明其他线程尝试解锁，这是不允许的，锁的释放只能由持有锁的线程完成
          	// 此处抛出监视器状态异常，与ReentrantLock行为是一样的
          	// 所以为什么推荐finally块中的释放锁操作需要用判断锁的持有者来包裹
            if (opStatus == null) {
                IllegalMonitorStateException cause = new IllegalMonitorStateException("attempt to unlock lock, not locked by current thread by node id: "
                        + id + " thread-id: " + threadId);
                result.tryFailure(cause);
                return;
            }
            
          	// 此处有个疑问，如果opStatus返回false呢？可重入锁，还没完全释放就把看门狗杀了？
          	// 还是说opStatus代表的不是lua脚本的返回值？
            cancelExpirationRenewal(threadId);
            result.trySuccess(null);
        });

        return result;
    }
```

## redlock理论

### 理论部分

分布式锁的几个要求，其中**防死锁、不乱抢、可重入**等需求通过设置过期时间、释放时判断、锁状态计数器可以保证，但是**高可用、独占性**这两个需求并不容易同时满足。

为什么说高可用、独占性这两个需求并不好同时满足呢，就因为redis的高可用架构是异步复制的，实现的是CAP中的AP。

**为了保证高可用**，可以采取主从架构对外提供服务，但是由于是异步复制，线程A在主节点加锁成功后，即可进入临界区，此时，数据可能还没有同步到从节点，如果此时主节点宕机，进行主从切换后，从节点没有线程A的锁信息，线程B也可以加锁成功，**独占性不满足**。

在只有一台机器或者是同步复制的情况下才能保证保证锁的独占性，单点情况不做考虑，同步复制虽然能够保证独占性，但是CP系统是不能保证高可用的，只要有节点没法完成复制则数据无法写入。

为了解决这一问题，redis之父引入了redlock算法。

1. 准备大于等于3的奇数台（奇数个节点与偶数个节点相比，能够实现相同的可用性但是节点数更少）的独立的redis节点，这些节点都是master，没有slave（有slave则存在复制与切换）
2. 同时向这些节点发送加锁请求，只有对有超过半数以上的节点加锁成功，则进入临界区
3. 加锁成功后，锁的持续时间 = 申请占用总时间 - 加锁花费的时间 - 时间漂移量
4. 为了避免在不可用的节点上浪费大量时间，需要对每个加锁时间设置超时时间，一般这个等待时间要远远小于锁占用时间，比如申请占用10s，等待超时时间配置为5-50ms比较合适
5. 一旦获取锁失败，则必须向所有节点发送释放锁请求（可能由于网络问题，节点加锁成功了但是超时失败了）
6. 正常释放锁时也应该向所有节点发起释放请求

### redlock崩溃恢复

redlock能够在高可用的情况下尽量保证独占性，因为只需要超过半数节点加锁成功，则认为成功，允许宕机一部分节点。redlock需要持久化来保证服务在宕机之后重启还能保证锁的独占性

1. 如果没有持久化机制，一旦某个加了锁节点宕机，恢复后将不存在加锁信息，如果此时未加锁的节点恰好大于一半，在高并发下，其他线程很有可能加锁成功，独占性不满足了
2. 采用aof持久化机制，需要进行取舍，如果采用默认1s刷盘的机制也会有上述问题，但是被缩短到1s以内，如果采用alawys机制，每次命令都进行刷盘，虽然不会有问题，但是性能会受到较大的影响
3. 或者在节点宕机后等待ttl时间（这个时间是锁的过期时间，但是如果有锁续期则这个时间不好确定）后才将这个节点重启（直接重启也行，只要在ttl时间内不提供服务就行），这样就不会影响到锁的独占性，但是此时redlock的高可用会受到一些影响，如果再多挂掉一些机器则整个集群无法提供服务了。

